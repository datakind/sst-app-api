"""API functions related to data.
"""

from typing import Annotated, Any, Union, Tuple
from fastapi import APIRouter, Depends, HTTPException, status
from pydantic import BaseModel
from sqlalchemy import and_
import uuid
import datetime
from sqlalchemy.orm import Session
from sqlalchemy.future import select

from ..utilities import (
    has_access_to_inst_or_err,
    has_full_data_access_or_err,
    BaseUser,
    model_owner_and_higher_or_err,
    prepend_env_prefix,
    uuid_to_str,
    str_to_uuid,
    uuids_to_strs,
    # DataSource
)

from ..database import get_session, local_session, BatchTable, FileTable

from ..gcsutil import list_blobs_in_folder, download_file

router = APIRouter(
    prefix="/institutions",
    tags=["data"],
)


class BatchCreationRequest(BaseModel):
    """The Batch creation request."""

    # Must be unique within an institution to avoid confusion
    name: str
    description: Union[str, None] = None
    # Disabled data means it is no longer in use or not available for use.
    batch_disabled: bool = False


class BatchInfo(BaseModel):
    """The Batch Data object that's returned."""

    batch_id: str
    inst_id: str
    file_ids: set[str]
    # Must be unique within an institution to avoid confusion
    name: str
    description: Union[str, None] = None
    # User id of uploader or person who triggered this data ingestion.
    creator: str
    # Deleted data means this batch has a pending deletion request and can no longer be used.
    deleted: bool = False
    # Completed batches means this batch is ready for use. Completed batches will
    # trigger notifications to Datakind.
    # Can be modified after completion, but this information will not re-trigger
    # notifications to Datakind.
    completed: bool = False
    # Date in form YYMMDD. Deletion of a batch will apply to all files in a batch,
    # unless the file is present in other batches.
    deletion_request_time: Union[str, None] = None
    # TODO: update to be compatible with sql datetime field
    created_date: str


class DataInfo(BaseModel):
    """The Data object that's returned. Generally maps to a file, but technically maps to a GCS blob."""

    # Must be unique within an institution to avoid confusion.
    name: str
    data_id: str
    # The batch(es) that this data is present in.
    batch_ids: set[str]
    inst_id: str
    # Size to the nearest MB.
    size_mb: int
    description: Union[str, None] = None
    # User id of uploader or person who triggered this data ingestion. For SST generated files, this field would be null.
    uploader: Union[str, None] = None
    # Can be PDP_SFTP, MANUAL_UPLOAD etc.
    source: str  # DataSource = DataSource.UNKNOWN
    # Deleted data means this file has a pending deletion request or is deleted and can no longer be used.
    deleted: bool = False
    # Date in form YYMMDD
    deletion_request_time: Union[str, None] = None
    # How long to retain the data.
    # By default (None) -- it is deleted after a successful run. For training dataset it
    # is deleted after the trained model is approved. For inference input, it is deleted
    # after the inference run occurs. For inference output, it is retained indefinitely
    # unless an ad hoc deletion request is received. The type of data is determined by
    # the storage location.
    retention_days: Union[int, None] = None
    # Whether the file was generated by SST. (e.g. was it input or output)
    sst_generated: bool
    # Whether the file was validated (in the case of input) or approved (in the case of output).
    valid: bool
    # TODO: update to be compatible with sql datetime field
    uploaded_date: str


# Data related operations. Input files mean files sourced from the institution. Output files are generated by SST.


@router.get("/{inst_id}/input", response_model=tuple[list[DataInfo], list[BatchInfo]])
def read_inst_all_input_files(
    inst_id: str,
    current_user: Annotated[BaseUser, Depends()],
    sql_session: Annotated[Session, Depends(get_session)],
) -> Any:
    """Returns top-level overview of training input data (date uploaded, size, file names etc.).

    Only visible to data owners of that institution or higher.

    Args:
        current_user: the user making the request.
    """
    has_access_to_inst_or_err(inst_id, current_user)
    has_full_data_access_or_err(current_user, "input data")
    # Datakinders can see unapproved files as well.
    if current_user.is_datakinder:
        list_blobs_in_folder(prepend_env_prefix(inst_id), "input/")
    else:
        list_blobs_in_folder(prepend_env_prefix(inst_id), "input/approved")
    return []


@router.get("/{inst_id}/output", response_model=tuple[list[DataInfo], list[BatchInfo]])
def read_inst_all_output_files(
    inst_id: str,
    current_user: Annotated[BaseUser, Depends()],
    sql_session: Annotated[Session, Depends(get_session)],
) -> Any:
    """Returns top-level overview of input data (date uploaded, size, file names etc.) and batch info.

    Only visible to data owners of that institution or higher.

    Args:
        current_user: the user making the request.
    """
    has_access_to_inst_or_err(inst_id, current_user)
    has_full_data_access_or_err(current_user, "output data")

    if current_user.is_datakinder:
        list_blobs_in_folder(prepend_env_prefix(inst_id), "output/")
    else:
        list_blobs_in_folder(prepend_env_prefix(inst_id), "output/validated")
    return []


@router.post("/{inst_id}/batch/", response_model=BatchInfo)
def create_batch(
    inst_id: str,
    req: BatchCreationRequest,
    current_user: Annotated[BaseUser, Depends()],
    sql_session: Annotated[Session, Depends(get_session)],
) -> Any:
    """Create a new batch.

    Args:
        current_user: the user making the request.
    """
    has_access_to_inst_or_err(inst_id, current_user)
    model_owner_and_higher_or_err(current_user, "batch")
    local_session.set(sql_session)
    query_result = (
        local_session.get()
        .execute(select(BatchTable).where(BatchTable.name == req.name))
        .all()
    )
    if len(query_result) == 0:
        local_session.get().add(
            BatchTable(
                name=req.name,
                inst_id=str_to_uuid(inst_id),
                description=req.description,
                creator=current_user.user_id,
            )
        )
        local_session.get().commit()
        query_result = (
            local_session.get()
            .execute(select(BatchTable).where(BatchTable.name == req.name))
            .all()
        )
        if not query_result:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Database write of the batch creation failed.",
            )
        elif len(query_result) > 1:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Database write of the batch created duplicate entries.",
            )
    if len(query_result) > 1:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Batch duplicates found.",
        )
    return {
        "batch_id": uuid_to_str(query_result[0][0].id),
        "inst_id": query_result[0][0].inst_id,
        "name": query_result[0][0].name,
        "description": query_result[0][0].description,
        "file_names": [],
        "creator": query_result[0][0].creator,
        "deleted": False,
        "completed": False,
        "deletion_request_time": None,
        "created_date": query_result[0][0].time_created,
    }


@router.get(
    "/{inst_id}/batch/{batch_id}", response_model=tuple[list[DataInfo], BatchInfo]
)
def read_batch_info(
    inst_id: str,
    batch_id: str,
    current_user: Annotated[BaseUser, Depends()],
    sql_session: Annotated[Session, Depends(get_session)],
) -> Any:
    """Returns batch info and files in that batch.

    Only visible to users of that institution or Datakinder access types.

    Args:
        current_user: the user making the request.
    """
    has_access_to_inst_or_err(inst_id, current_user)
    has_full_data_access_or_err(current_user, "batch data")
    local_session.set(sql_session)
    query_result = (
        local_session.get()
        .execute(
            select(BatchTable).where(
                and_(
                    BatchTable.id == str_to_uuid(batch_id),
                    BatchTable.inst_id == str_to_uuid(inst_id),
                )
            )
        )
        .all()
    )
    if not query_result or len(query_result) == 0:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="No such batch exists.",
        )
    if len(query_result) > 1:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Batch duplicates found.",
        )
    res = query_result[0][0]
    batch_info = {
        "batch_id": uuid_to_str(res.id),
        "inst_id": uuid_to_str(res.inst_id),
        "name": res.name,
        "description": res.description,
        "file_ids": uuids_to_strs(res.files),
        "creator": res.creator,
        "deleted": res.deleted,
        "completed": res.completed,
        "deletion_request_time": res.time_deleted,
        "created_date": res.time_created,
    }
    print(type(res.files))
    data_infos = []
    for elem in res.files:
        print(type(elem))
        data_infos.append(
            {
                "name": elem.name,
                "data_id": uuid_to_str(elem.id),
                "batch_ids": uuids_to_strs(elem.batches),
                "inst_id": uuid_to_str(elem.inst_id),
                "size_mb": elem.size_mb,
                "description": elem.description,
                "uploader": uuid_to_str(elem.uploader),
                "source": elem.source,
                "deleted": elem.deleted,
                "deletion_request_time": elem.deletion_request_time,
                "retention_days": elem.retention_days,
                "sst_generated": elem.sst_generated,
                "valid": elem.valid,
                "uploaded_date": elem.time_created,
            }
        )
    return (batch_info, data_infos)


@router.get("/{inst_id}/file/{file_id}", response_model=DataInfo)
def read_file_info(
    inst_id: str,
    file_id: str,
    current_user: Annotated[BaseUser, Depends()],
    sql_session: Annotated[Session, Depends(get_session)],
) -> Any:
    """Returns details on a given file.

    Only visible to users of that institution or Datakinder access types.

    Args:
        current_user: the user making the request.
    """
    has_access_to_inst_or_err(inst_id, current_user)
    has_full_data_access_or_err(current_user, "file data")
    local_session.set(sql_session)
    query_result = (
        local_session.get()
        .execute(
            select(FileTable).where(
                and_(
                    FileTable.id == str_to_uuid(file_id),
                    FileTable.inst_id == str_to_uuid(inst_id),
                )
            )
        )
        .all()
    )
    # This should only result in a match of a single file.
    if not query_result or len(query_result) == 0:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="File not found.",
        )
    if len(query_result) > 1:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="File duplicates found.",
        )
    res = query_result[0][0]
    return {
        "name": res.name,
        "data_id": uuid_to_str(res.id),
        "batch_ids": uuids_to_strs(res.batches),
        "inst_id": uuid_to_str(res.inst_id),
        "size_mb": res.size_mb,
        "description": res.description,
        "uploader": uuid_to_str(res.uploader),
        "source": res.source,
        "deleted": res.deleted,
        "deletion_request_time": res.deletion_request_time,
        "retention_days": res.retention_days,
        "sst_generated": res.sst_generated,
        "valid": res.valid,
        "uploaded_date": res.time_created,
    }


@router.get("/{inst_id}/output/{file_id}", response_model=DataInfo)
def download_inst_file(
    inst_id: str,
    file_id: str,
    current_user: Annotated[BaseUser, Depends()],
    sql_session: Annotated[Session, Depends(get_session)],
) -> Any:
    """Enables download of approved output files.

    Only visible to users of that institution or Datakinder access types.

    Args:
        current_user: the user making the request.
    """
    has_access_to_inst_or_err(inst_id, current_user)
    has_full_data_access_or_err(current_user, "file data")
    local_session.set(sql_session)
    query_result = (
        local_session.get()
        .execute(
            select(FileTable).where(
                and_(
                    FileTable.id == str_to_uuid(file_id),
                    FileTable.inst_id == str_to_uuid(inst_id),
                )
            )
        )
        .all()
    )
    # This should only result in a match of a single file.
    if not query_result or len(query_result) == 0:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="File not found.",
        )
    if len(query_result) > 1:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="File duplicates found.",
        )
    res = query_result[0][0]
    if res.deleted:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="File has been deleted.",
        )
    if not res.sst_generated:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Only SST generated files can be downloaded.",
        )
    if res.valid or current_user.is_datakinder:
        # download the file
        bucket_name = prepend_env_prefix(str(res.inst_id))
        file_name = "output/approved/" + res.name
        dest = (
            "Downloads/" + res.name
        )  # xxx TODO update?? Do we want to use signed url for downloads?
        try:
            download_file(bucket_name, file_name, dest)
        except ValueError as e:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="File not found:" + str(e),
            )
    else:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="File not yet approved by Datakind. Cannot be downloaded.",
        )
    res = query_result[0][0]
    return {
        "name": res.name,
        "data_id": uuid_to_str(res.id),
        "batch_ids": uuids_to_strs(res.batches),
        "inst_id": uuid_to_str(res.inst_id),
        "size_mb": res.size_mb,
        "description": res.description,
        "uploader": uuid_to_str(res.uploader),
        "source": res.source,
        "deleted": res.deleted,
        "deletion_request_time": res.deletion_request_time,
        "retention_days": res.retention_days,
        "sst_generated": res.sst_generated,
        "valid": res.valid,
        "uploaded_date": res.time_created,
    }


@router.post("/{inst_id}/input/{batch_id}/")
def upload_file(
    inst_id: int, batch_id: int, current_user: Annotated[BaseUser, Depends()]
) -> Any:
    """Add new data from local filesystem.

    Args:
        current_user: the user making the request.
    """
    has_access_to_inst_or_err(inst_id, current_user)
    model_owner_and_higher_or_err(current_user, "model training data")
    # generate_upload_signed_url(str(inst_id), f"training_data/FOO.csv")
    # TODO: make the POST call to the upload url with the file.
    # Update or create batch.


@router.post("/{inst_id}/input/{batch_id}/pdp_sftp/")
def pull_pdp_sftp(inst_id: int, current_user: Annotated[BaseUser, Depends()]) -> Any:
    """Add new data from PDP directly.

    This post function triggers a file request to PDP's SFTP server.

    Args:
        current_user: the user making the request.
    """
    has_access_to_inst_or_err(inst_id, current_user)
    model_owner_and_higher_or_err(current_user, "data")
    # TODO: call function that handles PDP SFTP request here.
