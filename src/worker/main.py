"""Main file for the SST Worker."""

import numpy as np
import logging
from typing import Any, Annotated
from fastapi import FastAPI, Depends, HTTPException, status
from fastapi.responses import FileResponse

from pydantic import BaseModel
from fastapi.security import OAuth2PasswordRequestForm
from .utilities import (
    get_sftp_bucket_name,
    StorageControl,
    fetch_institution_ids,
    split_csv_and_generate_signed_urls,
    fetch_upload_url,
    transfer_file,
)
from .config import sftp_vars, env_vars, startup_env_vars
from .authn import Token, get_current_username, check_creds, create_access_token
from datetime import timedelta
import os

# Set the logging
logging.basicConfig(format="%(asctime)s [%(levelname)s]: %(message)s")
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

app = FastAPI(
    servers=[
        # TODO: placeholders
        {"url": "https://stag.example.com", "description": "Staging environment"},
        {"url": "https://prod.example.com", "description": "Production environment"},
    ],
    root_path="/worker/api/v1",
)

# this uses api key to auth to backend api, but credentials to auth to this service


class PdpPullRequest(BaseModel):
    """Params for the PDP pull request."""

    placeholder: str | None = None


class PdpPullResponse(BaseModel):
    """Fields for the PDP pull response."""

    sftp_files: list[dict]
    pdp_inst_generated: list[Any]
    pdp_inst_not_found: list[Any]
    upload_status: dict

    class Config:
        json_encoders = {np.int64: lambda v: int(v)}


@app.on_event("startup")
def on_startup():
    print("Starting up app...")
    startup_env_vars()


# On shutdown, we have to cleanup the GCP database connections
@app.on_event("shutdown")
def shutdown_event():
    print("Performing shutdown tasks...")


# The following root paths don't have pre-authn.
@app.get("/")
def read_root() -> Any:
    """Returns the index.html file."""
    return FileResponse("src/worker/index.html")


@app.post("/token")
async def login_for_access_token(
    form_data: Annotated[OAuth2PasswordRequestForm, Depends()],
) -> Token:
    valid = check_creds(form_data.username, form_data.password)
    if not valid:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password",
            headers={"WWW-Authenticate": "Bearer"},
        )
    access_token_expires = timedelta(
        minutes=int(env_vars["ACCESS_TOKEN_EXPIRE_MINUTES"])
    )
    access_token = create_access_token(
        data={"sub": form_data.username}, expires_delta=access_token_expires
    )
    return Token(access_token=access_token, token_type="bearer")


def sftp_helper(storage_control: StorageControl, sftp_source_filenames: list) -> list:
    """
    For each source file in sftp_source_filenames, copies the file from the SFTP
    server to GCS. The destination filename is automatically generated by prefixing
    the base name of the source file with "processed_".

    Args:
        storage_control (StorageControl): An instance with a method `copy_from_sftp_to_gcs`.
        sftp_source_filenames (list): A list of file paths on the SFTP server.
    """
    num_files = len(sftp_source_filenames)
    logger.info(f"Starting sftp_helper for {num_files} file(s).")
    all_blobs = []
    for sftp_source_filename in sftp_source_filenames:
        sftp_source_filename = sftp_source_filename["path"]
        if (
            sftp_source_filename
            == "./receive/AO1600pdp_AO1600_AR_DEIDENTIFIED_STUDYID_20250228030226.csv"
        ):
            logger.debug(f"Processing source file: {sftp_source_filename}")

            # Extract the base filename.
            base_filename = os.path.basename(sftp_source_filename)
            dest_filename = f"{base_filename}"
            logger.debug(f"Destination filename will be: {dest_filename}")

            try:
                storage_control.copy_from_sftp_to_gcs(
                    sftp_vars["SFTP_HOST"],
                    22,
                    sftp_vars["SFTP_USER"],
                    sftp_vars["SFTP_PASSWORD"],
                    sftp_source_filename,
                    get_sftp_bucket_name(env_vars["ENV"]),
                    dest_filename,
                )
                all_blobs.append(dest_filename)
                logger.info(
                    f"Successfully processed '{sftp_source_filename}' as '{dest_filename}'."
                )
            except Exception as e:
                logger.error(
                    f"Error processing '{sftp_source_filename}': {e}", exc_info=True
                )
    return all_blobs


@app.post("/execute-pdp-pull", response_model=PdpPullResponse)
async def execute_pdp_pull(
    req: PdpPullRequest,
    current_username: Annotated[str, Depends(get_current_username)],
    storage_control: Annotated[StorageControl, Depends(StorageControl)],
) -> Any:
    """Performs the PDP pull of the file."""

    storage_control.create_bucket_if_not_exists(
        get_sftp_bucket_name(env_vars["BUCKET_ENV"])
    )
    print(sftp_vars["SFTP_HOST"])
    files = storage_control.list_sftp_files(
        sftp_vars["SFTP_HOST"], 22, sftp_vars["SFTP_USER"], sftp_vars["SFTP_PASSWORD"]
    )

    all_blobs = sftp_helper(storage_control, files)
    print(f"It's all processed {all_blobs}")
    valid_inst_ids = []
    invalid_ids = []
    uploads = {}

    for blobs in all_blobs:
        logging.debug(f"Processing {blobs}")
        print(f"Processing {blobs}")
        signed_urls = split_csv_and_generate_signed_urls(
            bucket_name=get_sftp_bucket_name(env_vars["BUCKET_ENV"]),
            source_blob_name=blobs,
        )
        logging.info(f"Signed URls generated {signed_urls}")

        temp_valid_inst_ids, temp_invalid_ids = fetch_institution_ids(
            pdp_ids=list(signed_urls.keys()),
            backend_api_key=env_vars["BACKEND_API_KEY"],
            webapp_url=env_vars["WEBAPP_URL"],
        )

        valid_inst_ids.append(temp_valid_inst_ids)
        invalid_ids.append(temp_invalid_ids)

        if temp_valid_inst_ids:
            for ids in temp_valid_inst_ids:
                upload_url = fetch_upload_url(
                    file_name=blobs,
                    institution_id=temp_valid_inst_ids[ids],
                    webapp_url=env_vars["WEBAPP_URL"],
                    backend_api_key=env_vars["BACKEND_API_KEY"],
                )
                print(upload_url)

                transfer_status = transfer_file(
                    download_url=signed_urls[ids]["signed_url"],
                    upload_signed_url=upload_url,
                )
                uploads[str(ids)] = {
                    "file_name": signed_urls[ids]["file_name"].strip().strip('"'),
                    "status": (
                        transfer_status.strip()
                        if isinstance(transfer_status, str)
                        else transfer_status
                    ),
                }

    return {
        "sftp_files": files,
        "pdp_inst_generated": valid_inst_ids,
        "pdp_inst_not_found": invalid_ids,
        "upload_status": uploads,
    }
